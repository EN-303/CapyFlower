{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "\n",
        "#wget https://github.com/nyp-sit/iti121-2025s2/raw/refs/heads/main/L6/data/goldfish_v1.zip\n",
        "wget https://github.com/EN-303/CapyFlower/raw/refs/heads/main/assignment2.zip\n",
        "\n",
        "#mkdir -p datasets/assignment1/\n",
        "unzip assignment2.zip -d datasets/"
      ],
      "metadata": {
        "id": "XPG7IPZzJwQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BTQ6QpZz3kP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Model\n",
        "\n",
        "YOLOv11 comes with different sizes of pretrained models: yolo11n, yolo11s, .... They differs in terms of their sizes, inference speeds and mean average precision:\n",
        "\n",
        "<img src=\"https://github.com/nyp-sit/iti121-2025s2/blob/main/L6/assets/yolo11-models.png?raw=true\" width=\"70%\"/>\n",
        "\n",
        "\n",
        "We will use the small pretrained model yolo11s and finetune it on our custom dataset.\n"
      ],
      "metadata": {
        "id": "YgyWGNT4-MLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup the logging\n",
        "\n",
        "Ultralytics support logging to `wandb`, `comet.ml` and `tensorboard` and `mlflow` out of the box. Here we only enable wandb.\n",
        "\n",
        "You need to create an account at [`wandb`](https://wandb.ai) and get the API key from https://wandb.ai/authorize.\n",
        "\n",
        "*For mlflow users, you can refer to Ultralytics's mlflow integration here: https://docs.ultralytics.com/integrations/mlflow/*\n"
      ],
      "metadata": {
        "id": "pKnPWyDwUfvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import settings\n",
        "\n",
        "settings.update({\"wandb\": True,\n",
        "                 \"clearml\": False,\n",
        "                 \"comet\": False})"
      ],
      "metadata": {
        "id": "FbMwi27fRsyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "For a complete listing of train settings, you can see [here](https://docs.ultralytics.com/modes/train/#train-settings).\n",
        "\n",
        "You can also specify the type of data [augmentation](https://docs.ultralytics.com/modes/train/#augmentation-settings-and-hyperparameters)  you want as part of the train pipeline.\n",
        "\n",
        "You can monitor your training progress at wandb (the link is given in the train output below)\n"
      ],
      "metadata": {
        "id": "iq9gV4A1VHlq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "fj6nj10o0Oki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la datasets/assignment2/train/images | wc -l"
      ],
      "metadata": {
        "id": "zrnbwsmcpYE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwjnKk02x-cF"
      },
      "outputs": [],
      "source": [
        "from numpy import flipud\n",
        "from ultralytics import YOLO\n",
        "from ultralytics import settings\n",
        "\n",
        "model = YOLO(\"yolo11s.pt\")  # Load a pre-trained YOLO model\n",
        "\n",
        "#v2 - 1\n",
        "# result = model.train(data=\"datasets/assignment2/data.yaml\",\n",
        "#                      epochs=30,\n",
        "#                      save_period=1, #save at each epochs\n",
        "#                      batch=64,#64 images at once\n",
        "#                      device=0, #gpu 0\n",
        "#                      mosaic=1.0,\n",
        "#                      mixup=0.2,\n",
        "#                      scale=0.7,\n",
        "#                      fliplr=0.5,\n",
        "#                      hsv_s=0.8,  # chg s\n",
        "#                      hsv_v=0.5,  # chg v\n",
        "#                      project='assignment2_v2',\n",
        "#                      plots=True)\n",
        "\n",
        "# #v2 - 2\n",
        "# result = model.train(data=\"datasets/assignment2/data.yaml\",\n",
        "#                      epochs=50,\n",
        "#                      save_period=1, #save at each epochs\n",
        "#                      batch=64,#64 images at once\n",
        "#                      device=0, #gpu 0\n",
        "#                      mosaic=0.5,\n",
        "#                      mixup=0.0,\n",
        "#                      scale=0.5,\n",
        "#                      fliplr=0.5,\n",
        "#                      hsv_s=0.5,  # chg s\n",
        "#                      hsv_v=0.3,  # chg v\n",
        "#                      project='assignment2_v2',\n",
        "#                      plots=True)\n",
        "\n",
        "# #v2 - 3\n",
        "# result = model.train(data=\"datasets/assignment2/data.yaml\",\n",
        "#                      epochs=50,\n",
        "#                      save_period=1, #save at each epochs\n",
        "#                      batch=64,#64 images at once\n",
        "#                      device=0, #gpu 0\n",
        "#                      mosaic=0.5,\n",
        "#                      close_mosaic=10,\n",
        "#                      scale=0.5,\n",
        "#                      fliplr=0.5,\n",
        "#                      hsv_h=0.5,  # chg h\n",
        "#                      hsv_s=0.5,  # chg s\n",
        "#                      hsv_v=0.3,  # chg v - brightness\n",
        "#                      project='assignment2_v2',\n",
        "#                      plots=True)\n",
        "\n",
        "# #v2 - 4\n",
        "# result = model.train(data=\"datasets/assignment2/data.yaml\",\n",
        "#                      epochs=80,\n",
        "#                      save_period=1, #save at each epochs\n",
        "#                      batch=16,\n",
        "#                      device=0, #gpu 0\n",
        "#                      mosaic=0.5,\n",
        "#                      close_mosaic=10,\n",
        "#                      scale=0.5,\n",
        "#                      fliplr=0.5,\n",
        "#                      hsv_s=0.4,  # chg s\n",
        "#                      hsv_v=0.3,  # chg v - brightness\n",
        "#                      project='assignment2_v2',\n",
        "#                      plots=True)\n",
        "\n",
        "#v2 - 5\n",
        "result = model.train(data=\"datasets/assignment2/data.yaml\",\n",
        "                     epochs=80,\n",
        "                     save_period=1, #save at each epochs\n",
        "                     batch=64,\n",
        "                     device=0, #gpu 0\n",
        "                     mosaic=0.5,\n",
        "                     close_mosaic=10,\n",
        "                     scale=0.5,\n",
        "                     fliplr=0.5,\n",
        "                     hsv_s=0.4,  # chg s\n",
        "                     hsv_v=0.3,  # chg v - brightness\n",
        "                     project='assignment2_v2',\n",
        "                     plots=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can run the best model (using the best checkpoint) against the validation dataset to see the overall model performance on validation set.  \n",
        "\n",
        "You should see around `0.95` for `mAP50`, and `0.45` for `mAP50-95`."
      ],
      "metadata": {
        "id": "G-PxfxzMbAOS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-n6GRS5f2f05"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"assignment2_v2/train4/weights/best.pt\")\n",
        "validation_results = model.val(data=\"datasets/assignment2/data.yaml\", device=\"0\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO(\"assignment2_v2/train5/weights/best.pt\")\n",
        "validation_results = model.val(data=\"datasets/assignment2/data.yaml\", device=\"0\")"
      ],
      "metadata": {
        "id": "w5l7BTtzPBNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export and Deployment\n",
        "\n",
        "Your model is in pytorch format (.pt). You can export the model to various format, e.g. TorchScript, ONNX, OpenVINO, TensorRT, etc. depending on your use case, and deployment platform (e.g. CPU or GPU, etc)\n",
        "\n",
        "You can see the list of [supported formats](https://docs.ultralytics.com/modes/export/#export-formats)  and the option they support in terms of further optimization (such as imagesize, int8, half-precision, etc) in the ultralytics site.\n",
        "\n",
        "Ultralytics provide a utility function to benchmark your model using different supported formats automatically. You can uncomment the code in the following code cell to see the benchmark result. If you are benchmarking for CPU only, the change the `device=0` to `device='cpu'`.  \n",
        "\n",
        "**Beware: it will take quite a while to complete the benchmark**"
      ],
      "metadata": {
        "id": "bio2cKcnb-Z-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Usgkfg87bZBO"
      },
      "outputs": [],
      "source": [
        "# from ultralytics.utils.benchmarks import benchmark\n",
        "\n",
        "# # Benchmark on GPU (device=0 means the 1st GPU device)\n",
        "# benchmark(model=\"goldfish_v1/train/weights/best.pt\", data=\"datasets/data.yaml\", imgsz=640, half=False, device=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Export Train 4 model"
      ],
      "metadata": {
        "id": "sqi84SKtUrzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6PDNf5NaPch"
      },
      "outputs": [],
      "source": [
        "# model = YOLO(\"assignment2/train/weights/best.pt\")\n",
        "# exported_path = model.export(format=\"openvino\", int8=True)\n",
        "\n",
        "model = YOLO(\"assignment2_v2/train4/weights/best.pt\")\n",
        "exported_path = model.export(format=\"openvino\", int8=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "Let's test our model on some sample pictures. You can optionally specify the confidence threshold (e.g. `conf=0.5`), and the IoU (e.g. `iou=0.6`) for the NMS. The model will only output the bounding boxes of those detection that exceeds the confidence threshould and the IoU threshold.  "
      ],
      "metadata": {
        "id": "r11lPCYrfMKN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziKNFammhMxv"
      },
      "outputs": [],
      "source": [
        "import ultralytics\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "\n",
        "#source = 'https://raw.githubusercontent.com/nyp-sit/iti121-2025S2/refs/heads/main/L6/samples/goldfish_sample.jpg'\n",
        "source = 'https://github.com/EN-303/CapyFlower/raw/refs/heads/main/sample/combine_002.png'\n",
        "\n",
        "#model = YOLO(\"assignment2/train/weights/best_int8_openvino_model\", task='detect')\n",
        "model = YOLO(\"assignment2_v2/train4/weights/best.pt\", task='detect')\n",
        "\n",
        "result = model(source, conf=0.5, iou=0.6)\n",
        "\n",
        "# Visualize the results\n",
        "for i, r in enumerate(result):\n",
        "    print(r)\n",
        "    # Plot results image\n",
        "    im_bgr = r.plot()  # BGR-order numpy array\n",
        "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # RGB-order PIL image\n",
        "\n",
        "    # Show results to screen (in supported environments)\n",
        "    r.show()\n",
        "\n",
        "    # Save results to disk\n",
        "    r.save(filename=f\"results{i}.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the Model\n",
        "\n",
        "If you are training your model on Google Colab, you will download the exported OpenVINO model to a local PC. If you are training your model locally, then the exported model should already be on your local PC.\n",
        "\n",
        "Run the following code to zip up the OpenVINO folder and download to local PC."
      ],
      "metadata": {
        "id": "ygXaw7lxu8HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: If you encountered error message \"NotImplementedError: A UTF-8 locale is required. Got ANSI_X3.4-1968\", uncomment the following cell and run it.*\n"
      ],
      "metadata": {
        "id": "xTTV-ndvuOx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import locale\n",
        "# locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "_O-nmcYRuMIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "mv ./assignment2_v2/train4/weights/best_int8_openvino_model/ .\n",
        "zip -r assignment2_train4_openvino_model.zip best_int8_openvino_model\n",
        "\n",
        "# Now go to best_openvino_model to download the best_openvino_model.zip file"
      ],
      "metadata": {
        "id": "j_CI0ZBTteA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui8i_pB5sItu"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "We can also do real-time detection on a video or camera steram.\n",
        "\n",
        "The code below uses openCV library to display video in a window, and can only be run locally on a local laptop.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video File\n",
        "\n",
        "You need `OpenCV` to run the following code.  In your conda environment, install `opencv` for python using the following command:\n",
        "\n",
        "```\n",
        "pip3 install opencv-python\n",
        "```\n",
        "or\n",
        "```\n",
        "conda install opencv\n",
        "```\n",
        "\n",
        "Let's donwload the sample video file."
      ],
      "metadata": {
        "id": "MmzL3StT5_CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://github.com/EN-303/CapyFlower/raw/refs/heads/main/sample/capyflower.mp4"
      ],
      "metadata": {
        "id": "DNK13LNd87G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming and display video"
      ],
      "metadata": {
        "id": "2nvf5BvM9_q2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "model = YOLO(\"assignment2_v2/train4/weights/best.pt\", task='detect')\n",
        "\n",
        "video_path = \"capyflower.mp4\"  # Local video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "while cap.isOpened():\n",
        "    success, frame = cap.read()\n",
        "    if not success:\n",
        "        break\n",
        "\n",
        "    results = model(frame, device=\"cpu\")\n",
        "    annotated_frame = results[0].plot()\n",
        "\n",
        "    # Display frame in Colab\n",
        "    cv2_imshow(annotated_frame)\n",
        "\n",
        "cap.release()\n"
      ],
      "metadata": {
        "id": "FVU5CP1sLmb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detect and write to a video file"
      ],
      "metadata": {
        "id": "AZoRxssK98O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "# from tqdm import tqdm\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def write_video(video_in_filepath, video_out_filepath, model):\n",
        "    # Open the video file\n",
        "\n",
        "    video_reader = cv2.VideoCapture(video_in_filepath)\n",
        "\n",
        "    nb_frames = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_h = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    frame_w = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    fps = video_reader.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "    video_writer = cv2.VideoWriter(video_out_filepath,\n",
        "                            cv2.VideoWriter_fourcc(*'mp4v'),\n",
        "                            fps,\n",
        "                            (frame_w, frame_h))\n",
        "\n",
        "    # Loop through the video frames\n",
        "    for i in tqdm(range(nb_frames)):\n",
        "        # Read a frame from the video\n",
        "        success, frame = video_reader.read()\n",
        "\n",
        "        if success:\n",
        "            # Run YOLO inference on the frame on GPU Device 0\n",
        "            results = model(frame, conf=0.6, device=0)\n",
        "\n",
        "            # Visualize the results on the frame\n",
        "            annotated_frame = results[0].plot()\n",
        "\n",
        "            # Write the annotated frame\n",
        "            video_writer.write(annotated_frame)\n",
        "\n",
        "    video_reader.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    cv2.waitKey(1)\n"
      ],
      "metadata": {
        "id": "fqXYqdRC64mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# video_in_file = \"goldfish_480p_10s.mp4\"\n",
        "video_in_file = \"capyflower.mp4\"\n",
        "\n",
        "basename = Path(video_in_file).stem\n",
        "video_out_file = os.path.join(basename + '_train4_detected' + '.mp4')\n",
        "\n",
        "# model = YOLO(\"best_int8_openvino_model\", task=\"detect\")\n",
        "model = YOLO(\"assignment2_v2/train4/weights/best.pt\", task='detect')\n",
        "\n",
        "write_video(video_in_file, video_out_file, model)"
      ],
      "metadata": {
        "id": "oT_bq-gC8q2m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}